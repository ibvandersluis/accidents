accidents$driver_imd_decile = NULL
table(accidents$vehicle_imd_decile, accidents$fatal)
# vehicle_imd_decile is also empty and can be removed
accidents$vehicle_imd_decile = NULL
table(accidents$No_of_Vehicles_involved_unique_to_accident_index, accidents$number_of_vehicles)
# No_of_Vehicles_involved_unique_to_accident_index is identical to numer_of_vehicles and can be removed
accidents$No_of_Vehicles_involved_unique_to_accident_index = NULL
# Remove junk data from 'time' variable
accidents$time = substring(accidents$time, first = 12)
# The 'date' variable either shows a date in 2015 or an integer in the 40,000 range
# Unfortunately this means it cannot be relied upon for any useful information
# Remove date
accidents$date = NULL
# Remove NA rows
sum(is.na(accidents))
accidents = na.omit(accidents)
# Remove NA rows
sum(is.na(accidents))
str(accidents)
# Convert boolean response variable to logical
# accidents$fatal = as.logical(accidents$fatal)
accidents$fatal = as.integer(accidents$fatal)
# Convert $pedestrian_location and $pedestrian_movement to numerical
accidents$pedestrian_location = as.integer(accidents$pedestrian_location)
accidents$pedestrian_movement = as.integer(accidents$pedestrian_movement)
# Convert time
library(chron)
accidents$time = chron(times=accidents$time)
# Check data again
str(accidents)
# All features are numeric, check for significance of variables and if we can remove any
# Inpect correlations
cor(accidents)
# Remove NA rows
sum(is.na(accidents))
accidents = na.omit(accidents)
# All features are numeric, check for significance of variables and if we can remove any
# Inpect correlations
cor(accidents)
# Fit linear model
lm.fit = lm(fatal ~ ., data = accidents)
summary(lm.fit)
# Remove features with p-value > 0.05
accidents$carriageway_hazards = NULL
accidents$special_conditions_at_site = NULL
accidents$weather_conditions = NULL
accidents$pedestrian_crossing.human_control = NULL
accidents$age_band_of_driver = NULL
accidents$road_type = NULL
accidents$number_of_vehicles = NULL
accidents$driver_home_area_type = NULL
accidents$age_of_vehicle = NULL
accidents$propulsion_code = NULL
accidents$X1st_point_of_impact = NULL
accidents$was_vehicle_left_hand_drive. = NULL
# accidents$hit_object_off_carriageway = NULL
# Refit and check p-values again
lm.fit = lm(fatal ~ ., data = accidents)
summary(lm.fit)
# Remove features with p-value > 0.05
accidents$vehicle_type = NULL
# accidents$towing_and_articulation = NULL
# Check again
lm.fit = lm(fatal ~ ., data = accidents)
summary(lm.fit)
# Perform best subsets selection
library(leaps)
regfit.full = regsubsets(fatal ~ ., accidents, nvmax=25)
reg.summary= summary(regfit.full)
reg.summary$rsq
# Plot RSS
plot(reg.summary$rss, xlab="Number of Variables", ylab="RSS", type="l")
# Plot Adjusted Rsq
plot(reg.summary$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="l")
which.max(reg.summary$adjr2)
points(25, reg.summary$adjr2[25], col="red", cex=2, pch=20)
# Plot Cp
plot(reg.summary$cp, xlab="Number of Variables", ylab="Cp", type="l")
which.min(reg.summary$cp)
points(25, reg.summary$cp[25], col="red", cex=2, pch=20)
# Plot BIC
plot(reg.summary$bic, xlab="Number of Variables", ylab="BIC", type="l")
which.min(reg.summary$bic)
points(17, reg.summary$bic[17], col="red", cex=2, pch=20)
plot(regfit.full, scale="r2")
plot(regfit.full, scale="adjr2")
plot(regfit.full, scale="Cp")
plot(regfit.full, scale="bic")
# Export CSV of prepared data
write.csv(accidents, "prep.csv", row.names = F)
# Export as dataframe that keeps formatting
saveRDS(accidents, file="prep.rds")
# Save history
savehistory()
# Clear workspace
rm(list=ls())
# Load library
library(class)
# Load data
accidents = readRDS("prep.rds")
summary(accidents)
attach(accidents)
# Scale the data except for the qualitative response variable
scaled = scale(accidents[, -26])
var(accidents[,1])
var(accidents[,2])
var(scaled[,1])
var(scaled[,2])
# Create test set
# Get random sample to test on
set.seed(42)
testRows = sample(1:nrow(accidents), 0.1*nrow(accidents))
train.X = scaled[-testRows,]
test.X = scaled[testRows,]
train.Y = fatal[-testRows]
test.Y = fatal[testRows]
# Fit kNN model
knn.pred = knn(train.X, test.X, train.Y, k=1)
mean(test.Y != knn.pred)
mean(test.Y != 0)
table(knn.pred, fatal)
table(knn.pred, test.Y)
24/(24 + 49)
# Refit with k=3
knn.pred = knn(train.X, test.X, train.Y, k=3)
mean(test.Y != knn.pred)
mean(test.Y != 0)
# Lower test error than k=1
mean(test.Y != 0)
table(knn.pred, test.Y)
12/(12 + 61)
# Refit with k=5
knn.pred = knn(train.X, test.X, train.Y, k=5)
mean(test.Y != knn.pred)
mean(test.Y != 0)
table(knn.pred, test.Y)
# Our accuracy is amazing-ish, but we
6/(6 + 67)
# Refit with k=7
knn.pred = knn(train.X, test.X, train.Y, k=7)
mean(test.Y != knn.pred)
# Incredibly low test error, probably means it barely predicted any crashes
mean(test.Y != 0)
table(knn.pred, test.Y)
3/(3 + 70)
# Save history
savehistory()
# Clear workspace
rm(list=ls())
# Load data
accidents = readRDS("prep.rds")
attach(accidents)
summary(accidents)
# Create test set
# Get random sample to test on
set.seed(42)
testRows = sample(1:nrow(accidents), 0.1*nrow(accidents))
accidents.train = accidents[-testRows,]
accidents.test = accidents[testRows,]
fatal.test = fatal[testRows]
# Fit logistic regression model
glm.fit = glm(fatal ~ ., data=accidents, family=binomial, subset=-testRows)
glm.probs = predict(glm.fit, accidents.test, type="response")
glm.preds = rep(0,10132)
glm.preds[glm.probs>.5] = 1
table(glm.preds, fatal.test)
# Try with threshold = 0.4
glm.preds[glm.probs>.4] = 1
table(glm.preds, fatal.test)
# Try with threshold = 0.3
glm.preds[glm.probs>.3] = 1
table(glm.preds, fatal.test)
# Try with threshold = 0.2
glm.preds[glm.probs>.2] = 1
table(glm.preds, fatal.test)
# Try with threshold = 0.1
glm.preds[glm.probs>.1] = 1
table(glm.preds, fatal.test)
table(glm.preds, fatal.test)
# Try with threshold = 0.05
glm.preds[glm.probs>.05] = 1
table(glm.preds, fatal.test)
# Try with threshold = 0.5
glm.preds = rep(0,10132)
glm.preds[glm.probs>.5] = 1
table(glm.preds, fatal.test)
# Try with threshold = 0.3
glm.preds = rep(0,10132)
glm.preds[glm.probs>.3] = 1
table(glm.preds, fatal.test)
# Try with threshold = 0.2
glm.preds = rep(0,10132)
glm.preds[glm.probs>.2] = 1
table(glm.preds, fatal.test)
# Try with threshold = 0.1
glm.preds = rep(0,10132)
glm.preds[glm.probs>.1] = 1
table(glm.preds, fatal.test)
# Try with threshold = 0.05
glm.preds = rep(0,10132)
glm.preds[glm.probs>.05] = 1
table(glm.preds, fatal.test)
# Try with threshold = 0.05
glm.preds = rep(0,10132)
glm.preds[glm.probs>.04] = 1
table(glm.preds, fatal.test)
glm.preds[glm.probs>.03] = 1
table(glm.preds, fatal.test)
# Try with threshold = 0.02
glm.preds = rep(0,10132)
glm.preds[glm.probs>.02] = 1
table(glm.preds, fatal.test)
# We successfully predict 30 of 71 fatal accidents
# With a lot of incorrect predictions as well...
# Predict 695, of which 30 are correct
30/(30 + 665)
# Try with threshold = 0.01
glm.preds = rep(0,10132)
glm.preds[glm.probs>.01] = 1
table(glm.preds, fatal.test)
41/(1794 + 41)
# Try with threshold = 0.009
glm.preds = rep(0,10132)
glm.preds[glm.probs>.009] = 1
table(glm.preds, fatal.test)
44/(44 + 2053)
# 2% accuracy
# Try with threshold = 0.007
glm.preds = rep(0,10132)
glm.preds[glm.probs>.007] = 1
table(glm.preds, fatal.test)
# Try with threshold = 0.007
glm.preds = rep(0,10132)
glm.preds[glm.probs>.005] = 1
table(glm.preds, fatal.test)
glm.preds[glm.probs>.002] = 1
table(glm.preds, fatal.test)
# Save history
savehistory()
# Clear workspace
rm(list=ls())
# Load tree library
library(tree)
# Load data
accidents = readRDS("prep.rds")
str(accidents)
attach(accidents)
# Make response variable logical
accidents$fatal = as.factor(accidents$fatal)
str(accidents)
# Turning variables into factors improves misclassification rate
accidents$towing_and_articulation = as.factor(accidents$towing_and_articulation)
accidents$vehicle_manoeuvre = as.factor(accidents$vehicle_manoeuvre)
accidents$vehicle_location.restricted_lane = as.factor(accidents$vehicle_location.restricted_lane)
accidents$junction_location = as.factor(accidents$junction_location)
accidents$skidding_and_overturning = as.factor(accidents$skidding_and_overturning)
accidents$hit_object_in_carriageway = as.factor(accidents$hit_object_in_carriageway)
accidents$vehicle_leaving_carriageway = as.factor(accidents$vehicle_leaving_carriageway)
accidents$hit_object_off_carriageway = as.factor(accidents$hit_object_off_carriageway)
accidents$journey_purpose_of_driver = as.factor(accidents$journey_purpose_of_driver)
accidents$sex_of_driver = as.factor(accidents$sex_of_driver)
accidents$day_of_week = as.factor(accidents$day_of_week)
accidents$X1st_road_class = as.factor(accidents$X1st_road_class)
accidents$junction_detail = as.factor(accidents$junction_detail)
accidents$junction_control = as.factor(accidents$junction_control)
accidents$X2nd_road_class = as.factor(accidents$X2nd_road_class)
accidents$pedestrian_crossing.physical_facilities = as.factor(accidents$pedestrian_crossing.physical_facilities)
accidents$light_conditions = as.factor(accidents$light_conditions)
accidents$road_surface_conditions = as.factor(accidents$road_surface_conditions)
accidents$urban_or_rural_area = as.factor(accidents$urban_or_rural_area)
accidents$pedestrian_location = as.factor(accidents$pedestrian_location)
accidents$pedestrian_movement = as.factor(accidents$pedestrian_movement)
str(accidents)
# Create test set
# Get random sample to test on
set.seed(42)
testRows = sample(1:nrow(accidents), 0.1*nrow(accidents))
accidents.train = accidents[-testRows,]
accidents.test = accidents[testRows,]
fatal.test = fatal[testRows]
# Fit classification tree
tree.accidents = tree(fatal ~ ., accidents.train, control=tree.control(nobs=91192, mindev=0.0008))
tree.preds = predict(tree.accidents, accidents.test, type="class")
table(tree.preds, fatal.test)
summary(tree.accidents)
# Implement pruning
set.seed(1)
cv.accidents = cv.tree(tree.accidents)
names(cv.accidents)
cv.accidents
# Try bagging
library(randomForest)
set.seed(1)
?randmForest
?randomForest()
# Fit classification tree
tree.accidents = tree(fatal ~ ., accidents[-testRows], control=tree.control(nobs=91192, mindev=0.0008))
# Fit classification tree
tree.accidents = tree(fatal ~ ., accidents[-testRows,], control=tree.control(nobs=91192, mindev=0.0008))
# Fit classification tree
tree.accidents = tree(fatal ~ ., accidents, subset=-testRows, control=tree.control(nobs=91192, mindev=0.0008))
tree.preds = predict(tree.accidents, accidents.test, type="class")
table(tree.preds, fatal.test)
summary(tree.accidents)
plot(tree.accidents)
# Implement pruning - doesn't work
set.seed(1)
cv.accidents = cv.tree(tree.accidents)
names(cv.accidents)
cv.accidents
cv.accidents = cv.tree(tree.accidents, FUN=prune.misclass)
names(cv.accidents)
cv.accidents
# Try bagging
library(randomForest)
set.seed(1)
?randomForest()
bag.accidents = randomForest(fatal ~ ., data=accidents.train, mtry=25, importance=T)
writeRDS(bag.accidents, "bag_accidents.rds")
saveRDS(bag.accidents, "bag_accidents.rds")
bag.accidents
yhat.bag = predict(bag.accidents, newdata=accidents.test)
plot(yhat.bag, fatal.test)
plot(yhat.bag, accidents.test)
table(yhat.bag, fatal.test)
# Try random forest
set.seed(1)
rf.accidents = randomForest(fatal ~ ., data=accidents.train, mtry=12, importance=T)
yhat.rf = predict(rf.accidents, newdata=accidents.test)
table(yhat.rf, fatal.test)
importance(rf.accidents)
plot(importance(rf.accidents))
varImpPlot(rf.accidents)
saveRDS(rf.accidents, "rf_accidents.rds")
# Try boosting
library(gbm)
set.seed(1)
boost.accidents = gbm(fatal ~ ., data=accidents.train, distribution="bernoulli", n.trees=1000, interaction.depth=9)
summary(boost.accidents)
summary(boost.accidents)
yhat.boost = predict(boost.accidents, newdata=accidents.test, ntrees=1000)
yhat.boost = predict(boost.accidents, newdata=accidents.test, n.trees=1000)
table(yhat.boost, fatal.test)
# Load tree library
library(tree)
# Fit classification tree
tree.accidents = tree(fatal ~ ., accidents, subset=-testRows, control=tree.control(nobs=91192, mindev=0.0008))
tree.preds = predict(tree.accidents, accidents.test, type="class")
table(tree.preds, fatal.test)
# Fit classification tree
tree.accidents = tree(fatal ~ ., accidents, subset=-testRows, control=tree.control(nobs=91192, mindev=0.0009))
tree.preds = predict(tree.accidents, accidents.test, type="class")
table(tree.preds, fatal.test)
# Fit classification tree
tree.accidents = tree(fatal ~ ., accidents, subset=-testRows, control=tree.control(nobs=91192, mindev=0.001))
tree.preds = predict(tree.accidents, accidents.test, type="class")
table(tree.preds, fatal.test)
# Fit classification tree
tree.accidents = tree(fatal ~ ., accidents, subset=-testRows, control=tree.control(nobs=91192, mindev=0.0007))
tree.preds = predict(tree.accidents, accidents.test, type="class")
table(tree.preds, fatal.test)
# Fit classification tree
tree.accidents = tree(fatal ~ ., accidents, subset=-testRows, control=tree.control(nobs=91192, mindev=0.0009))
tree.preds = predict(tree.accidents, accidents.test, type="class")
table(tree.preds, fatal.test)
summary(tree.accidents)
12/(12+ 61)
# Accurately predicted 16.4% of fatal accidents
12/(12 + 19)
png("decision_tree.png")
plot(tree.accidents)
# 38.7% of predictions were correct
summary(tree.accidents)
plot(tree.accidents)
plot(tree.accidents)
# Fit classification tree
tree.accidents = tree(fatal ~ ., accidents, subset=-testRows, control=tree.control(nobs=91192, mindev=0.0009))
tree.preds = predict(tree.accidents, accidents.test, type="class")
table(tree.preds, fatal.test)
# Create test set
# Get random sample to test on
set.seed(42)
# Fit classification tree
tree.accidents = tree(fatal ~ ., accidents, subset=-testRows, control=tree.control(nobs=91192, mindev=0.0009))
tree.preds = predict(tree.accidents, accidents.test, type="class")
table(tree.preds, fatal.test)
12/(12 + 61)
# Accurately predicted 16.4% of fatal accidents
12/(12 + 19)
# 38.7% of predictions were correct
summary(tree.accidents)
plot(tree.accidents)
plot(tree.accidents)
# Implement pruning - doesn't work
set.seed(1)
plot(tree.accidents)
plot(tree.accidents)
dev.off()
plot(tree.accidents)
png("decision_tree.png")
plot(tree.accidents)
dev.off()
png("decision_tree.png")
plot(tree.accidents)
text(tree.accidents, pretty=0)
dev.off()
png("tree.png")
plot(tree.accidents)
dev.off()
png("tree_text.png")
plot(tree.accidents)
text(tree.accidents, pretty=0)
dev.off()
table(yhat.bag, fatal.test)
# Successfully predicts 19 accidents
19/73
plot(tree.accidents)
text(tree.accidents, pretty=0)
table(yhat.rf, fatal.test)
plot(importance(rf.accidents))
# Try bagging
library(randomForest)
plot(importance(rf.accidents))
varImpPlot(rf.accidents)
savehistory()
# Clear workspace
rm(list=ls())
# Load library
library(class)
# Load data
accidents = readRDS("prep.rds")
summary(accidents)
attach(accidents)
# Scale the data except for the qualitative response variable
scaled = scale(accidents[, -26])
var(accidents[,1])
var(accidents[,2])
var(scaled[,1])
var(scaled[,2])
fix(scaled)
# Create test set
# Get random sample to test on
set.seed(42)
testRows = sample(1:nrow(accidents), 0.1*nrow(accidents))
train.X = scaled[-testRows,]
test.X = scaled[testRows,]
train.Y = fatal[-testRows]
test.Y = fatal[testRows]
# Fit kNN model with k=1
knn.pred = knn(train.X, test.X, train.Y, k=1)
mean(test.Y != knn.pred)
# Error rate is 0.9%
mean(test.Y != 0)
# Fatal crash rate is 0.7%
# We want to predict fatal crashes
# Our only concern is the ratio of total fatal crashes
# Compared to the fatal crashes we predicted correctly
table(knn.pred, test.Y)
24/(24 + 49)
# We predicted 32.8% of fatal crashes
24/(24 + 46)
24 + 46
# We predicted 32.8% of fatal crashes
24/(24 + 46)
# Refit with k=3
knn.pred = knn(train.X, test.X, train.Y, k=3)
mean(test.Y != knn.pred)
# Lower test error than k=1
mean(test.Y != 0)
table(knn.pred, test.Y)
# Refit with k=7
knn.pred = knn(train.X, test.X, train.Y, k=7)
mean(test.Y != knn.pred)
# Test error almost as low as the occurance rate of fatal crashes
mean(test.Y != 0)
table(knn.pred, test.Y)
3/(3 + 70)
# We predict 4.1% of fatal crashes
3/7
# Clear workspace
rm(list=ls())
# Load library
library(class)
# Load data
accidents = readRDS("prep.rds")
summary(accidents)
attach(accidents)
# Scale the data except for the qualitative response variable
scaled = scale(accidents[, -26])
var(accidents[,1])
var(accidents[,2])
var(scaled[,1])
var(scaled[,2])
fix(scaled)
# Create test set
# Get random sample to test on
set.seed(42)
testRows = sample(1:nrow(accidents), 0.1*nrow(accidents))
train.X = scaled[-testRows,]
test.X = scaled[testRows,]
train.Y = fatal[-testRows]
test.Y = fatal[testRows]
# Fit kNN model with k=1
knn.pred = knn(train.X, test.X, train.Y, k=1)
mean(test.Y != knn.pred)
# Error rate is 0.9%
mean(test.Y != 0)
# Fatal crash rate is 0.7%
# We want to predict fatal crashes
# Our only concern is the ratio of total fatal crashes
# Compared to the fatal crashes we predicted correctly
table(knn.pred, test.Y)
